{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HcyLopX8zh4T"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# `Prelude`"
      ],
      "metadata": {
        "id": "Osh4WeXAlsqn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ukTKJfClouu"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import urllib.parse  # to handle special characters in the title\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import networkx as nx\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def getJsonResponse(title):\n",
        "  # Define the components of the query\n",
        "  baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
        "  action = \"action=query\"\n",
        "  title = f\"titles={urllib.parse.quote(title)}\"\n",
        "  content = \"prop=revisions&rvprop=content\"\n",
        "  dataformat = \"format=json\"\n",
        "  rvslots = \"rvslots=main\"\n",
        "\n",
        "  # Construct the query URL\n",
        "  query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat, rvslots)\n",
        "\n",
        "  try:\n",
        "    # Make the request to Wikipedia API\n",
        "    wikiresponse = urllib.request.urlopen(query)\n",
        "\n",
        "    # Check if the HTTP status is OK (200)\n",
        "    if wikiresponse.getcode() != 200:\n",
        "      print(f\"Error: Received non-200 HTTP status code {wikiresponse.getcode()}\")\n",
        "      return None\n",
        "\n",
        "    wikidata = wikiresponse.read()\n",
        "\n",
        "    # Parse the JSON response\n",
        "    try:\n",
        "      wikiJson = json.loads(wikidata)\n",
        "    except json.JSONDecodeError:\n",
        "      print(\"Error: Failed to decode JSON response\")\n",
        "      return None\n",
        "\n",
        "    # Get the page from the JSON response\n",
        "    page = next(iter(wikiJson['query']['pages'].values()))  # extract the single page\n",
        "\n",
        "    # Check if the page has revisions and extract the latest wikitext content\n",
        "    if 'revisions' in page and len(page['revisions']) > 0:\n",
        "      wikitext = page['revisions'][0]['slots']['main']['*']  # extract wikitext from \"main\" slot\n",
        "      return wikitext\n",
        "    else:\n",
        "      #print(f\"Error: Page '{title}' does not contain revisions.\")\n",
        "      return None\n",
        "\n",
        "  except urllib.error.URLError as e:\n",
        "    print(f\"Network error: {e.reason}\")\n",
        "    return None\n",
        "  except Exception as e:\n",
        "    print(f\"Unexpected error: {str(e)}\")\n",
        "    return None\n",
        "\n",
        "## Convert the list to link titles e.g. John McCain (fictional) => John_McCain_(fictional)\n",
        "def extract_title_link(match):\n",
        "  # Regular expression to match the content between [[ and | (the first part of the link)\n",
        "  title = re.search(r'\\[\\[([^\\|\\]]+)', match)\n",
        "  if title:\n",
        "    # Replace all whitespaces in the title with underscores\n",
        "    return title.group(1).replace(\" \", \"_\")\n",
        "  else:\n",
        "    print(\"ERROR FINDING \", match)\n",
        "    return None\n",
        "\n",
        "def findLinks(wikipage):\n",
        "  pattern = r'\\[{2}[\\w\\-\\s\\(\\)]*\\|?[\\w\\s\\-\\(\\)]*\\]{2}' ## regex for finding links e.g.: [[John McCain (fictional)|John McCain]]\n",
        "  matches = re.findall(pattern, wikipage)\n",
        "  # Convert the list to a set to keep only unique matches\n",
        "  unique_matches = set(matches)\n",
        "\n",
        "  links = [extract_title_link(unique_match) for unique_match in unique_matches]\n",
        "  return links"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_links = [\"List of philosophers (A–C)\", \"List of philosophers (D–H)\", \"List of philosophers (I–Q)\", \"List of philosophers (R–Z)\"]\n",
        "title_links = []\n",
        "for wiki_link in wiki_links:\n",
        "  wiki_markup = getJsonResponse(wiki_link)\n",
        "  title_links.extend(findLinks(wiki_markup))\n",
        "#wiki_markup_a_c = getJsonResponse(\"List of philosophers (A–C)\")\n",
        "#wiki_markup_d_h = getJsonResponse(\"List of philosophers (D–H)\")\n",
        "#wiki_markup_i_q = getJsonResponse(\"List of philosophers (I–Q)\")\n",
        "#wiki_markup_r_z = getJsonResponse(\"List of philosophers (R–Z)\")\n",
        "\n",
        "#title_links = findLinks(wiki_markup_a_c)\n",
        "#title_links.extend(findLinks(wiki_markup_d_h))\n",
        "#title_links.extend(findLinks(wiki_markup_i_q))\n",
        "#title_links.extend(findLinks(wiki_markup_r_z))\n",
        "\n",
        "# Remove irrelevant links if they exist\n",
        "for unwanted in [\"List_of_philosophers\", \"Philosopher\", \"Stanford_Encyclopedia_of_Philosophy\", \"Encyclopedia_of_Philosophy\", \"Routledge_Encyclopedia_of_Philosophy\", \"The_Cambridge_Dictionary_of_Philosophy\", \"The_Oxford_Companion_to_Philosophy\"]:\n",
        "    if unwanted in title_links:\n",
        "        title_links.remove(unwanted)\n",
        "\n",
        "# Writing to files (warning this takes a while)\n",
        "invalid_links = []  # Track titles that could not be saved\n",
        "for title_link in title_links:\n",
        "  all_wikitext = getJsonResponse(title_link)\n",
        "  if not all_wikitext:\n",
        "    print(f\"Skipping '{title_link}' as it has no content.\")\n",
        "    invalid_links.append(title_link)  # Track invalid pages without modifying the list directly\n",
        "    continue\n",
        "  filename = f\"{title_link}.txt\"\n",
        "  with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(all_wikitext) # save all the wikitext into one file\n",
        "\n",
        "# Remove invalid links from title_links after iteration\n",
        "title_links = [link for link in title_links if link not in invalid_links]\n",
        "\n",
        "files = os.listdir()\n",
        "\n",
        "outgoing_links = {}\n",
        "for file in files:\n",
        "  if not file.endswith(\".txt\"): # makes sure only the txt files are opened\n",
        "    continue\n",
        "  with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "    wikipage = f.read()\n",
        "    wikipage_links = findLinks(wikipage)\n",
        "    withoutExtension = f.name.replace(\".txt\", \"\");\n",
        "\n",
        "    for link in wikipage_links:\n",
        "      if link in title_links:\n",
        "        outgoing_links.setdefault(withoutExtension, []).append(link)\n",
        "\n",
        "G = nx.DiGraph()\n",
        "for page in title_links:\n",
        "  with open(f\"{page}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "  word_count = len(content.split()) # splits the content into words by whitespace.\n",
        "  G.add_node(page, contentlength=word_count)\n",
        "  for link in outgoing_links.get(page) or []:\n",
        "    G.add_edge(page, link)\n",
        "\n",
        "## Removing isolated nodes\n",
        "isolated_nodes = [node for node, degree in dict(G.degree()).items() if degree == 0]\n",
        "if (isolated_nodes): G.remove_nodes_from(isolated_nodes)\n",
        "\n",
        "## Getting the largest connected component\n",
        "largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
        "S = G.subgraph(largest_cc).copy() # copying it to another variable\n",
        "S_undirected = S.to_undirected() # making it undirected"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyQyH8oTlrun",
        "outputId": "37757d18-7b4f-4765-e628-7552f76d05fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping 'José_Chasin' as it has no content.\n",
            "Skipping 'Dorion_Cairns' as it has no content.\n",
            "Skipping 'Mario_Calderoni' as it has no content.\n",
            "Skipping 'Abd_al-Jabiri' as it has no content.\n",
            "Skipping 'George_Campbell_(theologian)' as it has no content.\n",
            "Skipping 'Chiao_Hung' as it has no content.\n",
            "Skipping 'Joseph_Geyser' as it has no content.\n",
            "Skipping 'Hu_Hung' as it has no content.\n",
            "Skipping 'Raimundo_de_Farias_Brito' as it has no content.\n",
            "Skipping 'Jonathan_Earle_(philosopher)' as it has no content.\n",
            "Skipping 'Reinhardt_Grossmann' as it has no content.\n",
            "Skipping 'Sergei_Iosifovich_Hessen' as it has no content.\n",
            "Skipping 'Ronald_William_Hepburn' as it has no content.\n",
            "Skipping 'David_Walter_Hamlyn' as it has no content.\n",
            "Skipping 'Erich_Frank' as it has no content.\n",
            "Skipping 'Gadadhara_Bhattacharya' as it has no content.\n",
            "Skipping 'Jules_Lachelier' as it has no content.\n",
            "Skipping 'Márcio_Bilharinho_Naves' as it has no content.\n",
            "Skipping 'Arthur_Liebert' as it has no content.\n",
            "Skipping 'Minagawa_Kien' as it has no content.\n",
            "Skipping 'James_Oswald_(philosopher)' as it has no content.\n",
            "Skipping 'Pseudo-Grosseteste' as it has no content.\n",
            "Skipping 'Jørgen_Jørgensen_(philosopher)' as it has no content.\n",
            "Skipping 'Joseph_Kaspi' as it has no content.\n",
            "Skipping 'Liu_Tsung-chou' as it has no content.\n",
            "Skipping 'William_Penbygull' as it has no content.\n",
            "Skipping 'Frantisek_Weyr' as it has no content.\n",
            "Skipping 'Johannes_Sharpe' as it has no content.\n",
            "Skipping 'Boris_Petrovich_Vysheslavtsev' as it has no content.\n",
            "Skipping 'George_Turnball' as it has no content.\n",
            "Skipping 'Yi_Kan' as it has no content.\n",
            "Skipping 'Gabriel_Vazquez' as it has no content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prelimenary data analysis"
      ],
      "metadata": {
        "id": "cjvG2Qzn4JmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# General graph analysis\n",
        "print(G) # whole graph\n",
        "print(S) # largest connected component\n",
        "\n",
        "# Calculating total data size\n",
        "total_size = sum(os.path.getsize(f) for f in os.listdir() if f.endswith(\".txt\"))\n",
        "total_size_mb = total_size / (1024 * 1024)  # Convert bytes to MB\n",
        "print(f\"Total Size of Data: {total_size_mb:.2f} MB\")\n",
        "\n",
        "# Number of rows\n",
        "num_rows = len(title_links)\n",
        "print(f\"Number of Rows (Philosophers): {num_rows}\")\n",
        "\n",
        "# Number of nodes and links\n",
        "num_nodes = S.number_of_nodes()\n",
        "num_links = S.number_of_edges()\n",
        "print(f\"Number of Nodes: {num_nodes}\")\n",
        "print(f\"Number of Links: {num_links}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BGM681a4JXQ",
        "outputId": "afa6dd04-3905-4be8-c093-27f6fa610064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiGraph with 1471 nodes and 11205 edges\n",
            "DiGraph with 1465 nodes and 11202 edges\n",
            "Total Size of Data: 48.50 MB\n",
            "Number of Rows (Philosophers): 1734\n",
            "Number of Nodes: 1465\n",
            "Number of Links: 11202\n"
          ]
        }
      ]
    }
  ]
}