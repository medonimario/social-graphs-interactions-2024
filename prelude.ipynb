{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Osh4WeXAlsqn"
   },
   "source": [
    "# Prelude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains the following:\n",
    "1. Imports and defining helper functions\n",
    "2. Scraping data from wikipedia and downloads it\n",
    "3. Building the network either from\n",
    "    - (A) Downloaded files\n",
    "    - (B) Local pickle file (created from last time A was run)\n",
    "4. Simple prelimenary data analysis of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from wiki_utils import getJsonResponse, findLinks, build_graph_from_files\n",
    "\n",
    "# Set the directory to downloads\n",
    "DOWNLOADS_DIR = \"downloads\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping data\n",
    "Fetches philosopher data from their wikipedia pages and downloads the wikipedia pages as `{philosopher_name}.txt` in a `downloads/` directory.\n",
    "\n",
    ">**NOTES**\n",
    "> 1. This takes a while to run\n",
    "> 2. It deletes all previous content in `downloads`\n",
    "> 3. Downloads all pages but skips pages with *no content* or *redirects*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyQyH8oTlrun",
    "outputId": "37757d18-7b4f-4765-e628-7552f76d05fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1485 pages.\n",
      "Skipped 32 pages with no content.\n",
      "Skipped 249 redirect pages.\n"
     ]
    }
   ],
   "source": [
    "wiki_links = [\"List of philosophers (A–C)\", \"List of philosophers (D–H)\", \"List of philosophers (I–Q)\", \"List of philosophers (R–Z)\"]\n",
    "title_links = []\n",
    "\n",
    "verbose = False # Debug output during loops\n",
    "invalid_links = []  # Track titles that could not be saved\n",
    "redirect_links = []  # Track titles that are redirects\n",
    "\n",
    "# Delete and recreate the downloads directory\n",
    "if os.path.exists(DOWNLOADS_DIR):\n",
    "\tshutil.rmtree(DOWNLOADS_DIR)  # Delete the directory and all its contents\n",
    "os.makedirs(DOWNLOADS_DIR, exist_ok=True)  # Recreate the directory\n",
    "\n",
    "\n",
    "for wiki_link in wiki_links:\n",
    "  wiki_markup = getJsonResponse(wiki_link)\n",
    "  title_links.extend(findLinks(wiki_markup))\n",
    "\n",
    "# Remove irrelevant links if they exist\n",
    "for unwanted in [\"List_of_philosophers\", \"Philosopher\", \"Stanford_Encyclopedia_of_Philosophy\", \"Encyclopedia_of_Philosophy\", \"Routledge_Encyclopedia_of_Philosophy\", \"The_Cambridge_Dictionary_of_Philosophy\", \"The_Oxford_Companion_to_Philosophy\"]:\n",
    "\tif unwanted in title_links:\n",
    "\t\ttitle_links.remove(unwanted)\n",
    "\n",
    "# Writing to files (warning this takes a while)\n",
    "for title_link in title_links:\n",
    "\tall_wikitext = getJsonResponse(title_link)\n",
    "\tif not all_wikitext:\n",
    "\t\tif verbose: print(f\"Skipping '{title_link}' as it has no content.\")\n",
    "\t\tinvalid_links.append(title_link)  # Track invalid pages without modifying the list directly\n",
    "\t\tcontinue\n",
    "  \n",
    "  # Skip if the content starts with #REDIRECT\n",
    "\tif all_wikitext.strip().startswith(\"#REDIRECT\"):\n",
    "\t\tif verbose: print(f\"Skipping '{title_link}' as it is a redirect.\")\n",
    "\t\tredirect_links.append(title_link)  # Track redirect pages\n",
    "\t\tcontinue\n",
    "  \n",
    "\tfilename = os.path.join(DOWNLOADS_DIR, f\"{title_link}.txt\")\n",
    "\twith open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "\t\tfile.write(all_wikitext) # save all the wikitext into one file\n",
    "\n",
    "title_links = [link for link in title_links if link not in invalid_links + redirect_links]\n",
    "print(f\"Downloaded {len(title_links)} pages.\")\n",
    "print(f\"Skipped {len(invalid_links)} pages with no content.\")\n",
    "print(f\"Skipped {len(redirect_links)} redirect pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (A) Create from scratch \n",
    "From `downloads/`directory (saves local pickle file for later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with self-loops: ['Pierre_Teilhard_de_Chardin', 'Plato', 'Otto_Rühle', 'Shen_Buhai']\n"
     ]
    }
   ],
   "source": [
    "S = build_graph_from_files(DOWNLOADS_DIR)\n",
    "pickle.dump(S, open(\"graph.pkl\", \"wb\")) # Saved as local version for later use for (B)\n",
    "\n",
    "S_undirected = S.to_undirected()\n",
    "# Identify self-loop edges\n",
    "self_loops = list(nx.selfloop_edges(S_undirected))\n",
    "\n",
    "# Print nodes with self-loops\n",
    "print(f\"Nodes with self-loops: {[edge[0] for edge in self_loops]}\")\n",
    "\n",
    "# Remove self-loops\n",
    "S_undirected.remove_edges_from(self_loops)\n",
    "pickle.dump(S_undirected, open(\"graph_undirected.pkl\", \"wb\")) # Saved as local version for later use for (B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (B) OR use local version \n",
    "From `pickle` file created last time you ran (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph:\n",
    "S = pickle.load(open(\"graph.pkl\", \"rb\"))\n",
    "S_undirected = pickle.load(open(\"graph_undirected.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjvG2Qzn4JmN"
   },
   "source": [
    "## 4. Prelimenary data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in S: 1366\n",
      "Number of edges in S: 10864, and in S_undirected: 9026\n",
      "Size of downloaded data: 48.60 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of nodes in S: {S.number_of_nodes()}\")\n",
    "print(f\"Number of edges in S: {S.number_of_edges()}, and in S_undirected: {S_undirected.number_of_edges()}\")\n",
    "\n",
    "# Calculating total data size\n",
    "download_size = sum(os.path.getsize(os.path.join(DOWNLOADS_DIR, f)) for f in os.listdir(DOWNLOADS_DIR) if f.endswith(\".txt\"))\n",
    "download_size_mb = download_size / (1024 * 1024)  # Convert bytes to MB\n",
    "print(f\"Size of downloaded data: {download_size_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HcyLopX8zh4T"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
