{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Osh4WeXAlsqn"
   },
   "source": [
    "# `Prelude`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse  # to handle special characters in the title\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Fetching\n",
    "This takes a while to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ukTKJfClouu"
   },
   "outputs": [],
   "source": [
    "def getJsonResponse(title):\n",
    "  # Define the components of the query\n",
    "  baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "  action = \"action=query\"\n",
    "  title = f\"titles={urllib.parse.quote(title)}\"\n",
    "  content = \"prop=revisions&rvprop=content\"\n",
    "  dataformat = \"format=json\"\n",
    "  rvslots = \"rvslots=main\"\n",
    "\n",
    "  # Construct the query URL\n",
    "  query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat, rvslots)\n",
    "\n",
    "  try:\n",
    "    # Make the request to Wikipedia API\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "\n",
    "    # Check if the HTTP status is OK (200)\n",
    "    if wikiresponse.getcode() != 200:\n",
    "      print(f\"Error: Received non-200 HTTP status code {wikiresponse.getcode()}\")\n",
    "      return None\n",
    "\n",
    "    wikidata = wikiresponse.read()\n",
    "\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "      wikiJson = json.loads(wikidata)\n",
    "    except json.JSONDecodeError:\n",
    "      print(\"Error: Failed to decode JSON response\")\n",
    "      return None\n",
    "\n",
    "    # Get the page from the JSON response\n",
    "    page = next(iter(wikiJson['query']['pages'].values()))  # extract the single page\n",
    "\n",
    "    # Check if the page has revisions and extract the latest wikitext content\n",
    "    if 'revisions' in page and len(page['revisions']) > 0:\n",
    "      wikitext = page['revisions'][0]['slots']['main']['*']  # extract wikitext from \"main\" slot\n",
    "      return wikitext\n",
    "    else:\n",
    "      #print(f\"Error: Page '{title}' does not contain revisions.\")\n",
    "      return None\n",
    "\n",
    "  except urllib.error.URLError as e:\n",
    "    print(f\"Network error: {e.reason}\")\n",
    "    return None\n",
    "  except Exception as e:\n",
    "    print(f\"Unexpected error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "## Convert the list to link titles e.g. John McCain (fictional) => John_McCain_(fictional)\n",
    "def extract_title_link(match):\n",
    "  # Regular expression to match the content between [[ and | (the first part of the link)\n",
    "  title = re.search(r'\\[\\[([^\\|\\]]+)', match)\n",
    "  if title:\n",
    "    # Replace all whitespaces in the title with underscores\n",
    "    return title.group(1).replace(\" \", \"_\")\n",
    "  else:\n",
    "    print(\"ERROR FINDING \", match)\n",
    "    return None\n",
    "\n",
    "def findLinks(wikipage):\n",
    "  pattern = r'\\[{2}[\\w\\-\\s\\(\\)]*\\|?[\\w\\s\\-\\(\\)]*\\]{2}' ## regex for finding links e.g.: [[John McCain (fictional)|John McCain]]\n",
    "  matches = re.findall(pattern, wikipage)\n",
    "  # Convert the list to a set to keep only unique matches\n",
    "  unique_matches = set(matches)\n",
    "\n",
    "  links = [extract_title_link(unique_match) for unique_match in unique_matches]\n",
    "  return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating the network\n",
    "From fetched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyQyH8oTlrun",
    "outputId": "37757d18-7b4f-4765-e628-7552f76d05fe"
   },
   "outputs": [],
   "source": [
    "# Set the directory to downloads\n",
    "DOWNLOADS_DIR = \"downloads\"\n",
    "os.makedirs(DOWNLOADS_DIR, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "wiki_links = [\"List of philosophers (A–C)\", \"List of philosophers (D–H)\", \"List of philosophers (I–Q)\", \"List of philosophers (R–Z)\"]\n",
    "title_links = []\n",
    "for wiki_link in wiki_links:\n",
    "  wiki_markup = getJsonResponse(wiki_link)\n",
    "  title_links.extend(findLinks(wiki_markup))\n",
    "#wiki_markup_a_c = getJsonResponse(\"List of philosophers (A–C)\")\n",
    "#wiki_markup_d_h = getJsonResponse(\"List of philosophers (D–H)\")\n",
    "#wiki_markup_i_q = getJsonResponse(\"List of philosophers (I–Q)\")\n",
    "#wiki_markup_r_z = getJsonResponse(\"List of philosophers (R–Z)\")\n",
    "\n",
    "#title_links = findLinks(wiki_markup_a_c)\n",
    "#title_links.extend(findLinks(wiki_markup_d_h))\n",
    "#title_links.extend(findLinks(wiki_markup_i_q))\n",
    "#title_links.extend(findLinks(wiki_markup_r_z))\n",
    "\n",
    "# Remove irrelevant links if they exist\n",
    "for unwanted in [\"List_of_philosophers\", \"Philosopher\", \"Stanford_Encyclopedia_of_Philosophy\", \"Encyclopedia_of_Philosophy\", \"Routledge_Encyclopedia_of_Philosophy\", \"The_Cambridge_Dictionary_of_Philosophy\", \"The_Oxford_Companion_to_Philosophy\"]:\n",
    "    if unwanted in title_links:\n",
    "        title_links.remove(unwanted)\n",
    "\n",
    "# Writing to files (warning this takes a while)\n",
    "invalid_links = []  # Track titles that could not be saved\n",
    "for title_link in title_links:\n",
    "  all_wikitext = getJsonResponse(title_link)\n",
    "  if not all_wikitext:\n",
    "    print(f\"Skipping '{title_link}' as it has no content.\")\n",
    "    invalid_links.append(title_link)  # Track invalid pages without modifying the list directly\n",
    "    continue\n",
    "  filename = os.path.join(DOWNLOADS_DIR, f\"{title_link}.txt\")\n",
    "  with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(all_wikitext) # save all the wikitext into one file\n",
    "\n",
    "# Remove invalid links from title_links after iteration\n",
    "title_links = [link for link in title_links if link not in invalid_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(DOWNLOADS_DIR) if f.endswith(\".txt\")]\n",
    "\n",
    "outgoing_links = {}\n",
    "for file in files:\n",
    "  if not file.endswith(\".txt\"): # makes sure only the txt files are opened\n",
    "    continue\n",
    "  file_path = os.path.join(DOWNLOADS_DIR, file)\n",
    "  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    wikipage = f.read()\n",
    "    wikipage_links = findLinks(wikipage)\n",
    "    withoutExtension = os.path.splitext(file)[0]\n",
    "\n",
    "    for link in wikipage_links:\n",
    "      if link in title_links:\n",
    "        outgoing_links.setdefault(withoutExtension, []).append(link)\n",
    "\n",
    "G = nx.DiGraph()\n",
    "for page in title_links:\n",
    "  file_path = os.path.join(DOWNLOADS_DIR, f\"{page}.txt\")\n",
    "  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "  word_count = len(content.split()) # splits the content into words by whitespace.\n",
    "  G.add_node(page, contentlength=word_count)\n",
    "  for link in outgoing_links.get(page) or []:\n",
    "    G.add_edge(page, link)\n",
    "\n",
    "## Removing isolated nodes\n",
    "isolated_nodes = [node for node, degree in dict(G.degree()).items() if degree == 0]\n",
    "if (isolated_nodes): G.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "## Getting the largest connected component\n",
    "largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "S = G.subgraph(largest_cc).copy() # copying it to another variable\n",
    "S_undirected = S.to_undirected() # making it undirected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjvG2Qzn4JmN"
   },
   "source": [
    "### Prelimenary data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BGM681a4JXQ",
    "outputId": "afa6dd04-3905-4be8-c093-27f6fa610064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 1470 nodes and 11203 edges\n",
      "DiGraph with 1464 nodes and 11200 edges\n",
      "Total Size of Data: 45.90 MB\n",
      "Number of Rows (Philosophers): 1734\n",
      "Number of Nodes: 1464\n",
      "Number of Links: 11200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# General graph analysis\n",
    "print(G) # whole graph\n",
    "print(S) # largest connected component\n",
    "\n",
    "# Calculating total data size\n",
    "total_size = sum(os.path.getsize(os.path.join(DOWNLOADS_DIR, f\"{title_link}.txt\")) for f in os.listdir(DOWNLOADS_DIR) if f.endswith(\".txt\"))\n",
    "total_size_mb = total_size / (1024 * 1024)  # Convert bytes to MB\n",
    "print(f\"Total Size of Data: {total_size_mb:.2f} MB\")\n",
    "\n",
    "# Number of rows\n",
    "num_rows = len(title_links)\n",
    "print(f\"Number of Rows (Philosophers): {num_rows}\")\n",
    "\n",
    "# Number of nodes and links\n",
    "num_nodes = S.number_of_nodes()\n",
    "num_links = S.number_of_edges()\n",
    "print(f\"Number of Nodes: {num_nodes}\")\n",
    "print(f\"Number of Links: {num_links}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HcyLopX8zh4T"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
