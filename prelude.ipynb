{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Osh4WeXAlsqn"
   },
   "source": [
    "# `Prelude`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports and defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse  # to handle special characters in the title\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for data fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9ukTKJfClouu"
   },
   "outputs": [],
   "source": [
    "# Set the directory to downloads\n",
    "DOWNLOADS_DIR = \"downloads\"\n",
    "TITLE_LINKS_FILE = \"title_links.json\"\n",
    "\n",
    "def getJsonResponse(title):\n",
    "  # Define the components of the query\n",
    "  baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "  action = \"action=query\"\n",
    "  title = f\"titles={urllib.parse.quote(title)}\"\n",
    "  content = \"prop=revisions&rvprop=content\"\n",
    "  dataformat = \"format=json\"\n",
    "  rvslots = \"rvslots=main\"\n",
    "\n",
    "  # Construct the query URL\n",
    "  query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat, rvslots)\n",
    "\n",
    "  try:\n",
    "    # Make the request to Wikipedia API\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "\n",
    "    # Check if the HTTP status is OK (200)\n",
    "    if wikiresponse.getcode() != 200:\n",
    "      print(f\"Error: Received non-200 HTTP status code {wikiresponse.getcode()}\")\n",
    "      return None\n",
    "\n",
    "    wikidata = wikiresponse.read()\n",
    "\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "      wikiJson = json.loads(wikidata)\n",
    "    except json.JSONDecodeError:\n",
    "      print(\"Error: Failed to decode JSON response\")\n",
    "      return None\n",
    "\n",
    "    # Get the page from the JSON response\n",
    "    page = next(iter(wikiJson['query']['pages'].values()))  # extract the single page\n",
    "\n",
    "    # Check if the page has revisions and extract the latest wikitext content\n",
    "    if 'revisions' in page and len(page['revisions']) > 0:\n",
    "      wikitext = page['revisions'][0]['slots']['main']['*']  # extract wikitext from \"main\" slot\n",
    "      return wikitext\n",
    "    else:\n",
    "      #print(f\"Error: Page '{title}' does not contain revisions.\")\n",
    "      return None\n",
    "\n",
    "  except urllib.error.URLError as e:\n",
    "    print(f\"Network error: {e.reason}\")\n",
    "    return None\n",
    "  except Exception as e:\n",
    "    print(f\"Unexpected error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "## Convert the list to link titles e.g. John McCain (fictional) => John_McCain_(fictional)\n",
    "def extract_title_link(match):\n",
    "  # Regular expression to match the content between [[ and | (the first part of the link)\n",
    "  title = re.search(r'\\[\\[([^\\|\\]]+)', match)\n",
    "  if title:\n",
    "    # Replace all whitespaces in the title with underscores\n",
    "    return title.group(1).replace(\" \", \"_\")\n",
    "  else:\n",
    "    print(\"ERROR FINDING \", match)\n",
    "    return None\n",
    "\n",
    "def findLinks(wikipage):\n",
    "  pattern = r'\\[{2}[\\w\\-\\s\\(\\)]*\\|?[\\w\\s\\-\\(\\)]*\\]{2}' ## regex for finding links e.g.: [[John McCain (fictional)|John McCain]]\n",
    "  matches = re.findall(pattern, wikipage)\n",
    "  # Convert the list to a set to keep only unique matches\n",
    "  unique_matches = set(matches)\n",
    "\n",
    "  links = [extract_title_link(unique_match) for unique_match in unique_matches]\n",
    "  return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Fetching\n",
    "Purpose: Fetches philosopher data from their wikipedia pages and downloads the wikipedia pages as `{philosopher_name}.txt` in a `downloads/` directory.\n",
    "\n",
    "**NOTE: This takes a while to run! Also deletes all previous content in `downloads`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyQyH8oTlrun",
    "outputId": "37757d18-7b4f-4765-e628-7552f76d05fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'Chiao_Hung' as it has no content.\n",
      "Skipping 'José_Chasin' as it has no content.\n",
      "Skipping 'Abd_al-Jabiri' as it has no content.\n",
      "Skipping 'George_Campbell_(theologian)' as it has no content.\n",
      "Skipping 'Dorion_Cairns' as it has no content.\n",
      "Skipping 'Mario_Calderoni' as it has no content.\n",
      "Skipping 'Gadadhara_Bhattacharya' as it has no content.\n",
      "Skipping 'Erich_Frank' as it has no content.\n",
      "Skipping 'Ronald_William_Hepburn' as it has no content.\n",
      "Skipping 'Sergei_Iosifovich_Hessen' as it has no content.\n",
      "Skipping 'Hu_Hung' as it has no content.\n",
      "Skipping 'Joseph_Geyser' as it has no content.\n",
      "Skipping 'Jonathan_Earle_(philosopher)' as it has no content.\n",
      "Skipping 'David_Walter_Hamlyn' as it has no content.\n",
      "Skipping 'Reinhardt_Grossmann' as it has no content.\n",
      "Skipping 'Raimundo_de_Farias_Brito' as it has no content.\n",
      "Skipping 'Jules_Lachelier' as it has no content.\n",
      "Skipping 'Jørgen_Jørgensen_(philosopher)' as it has no content.\n",
      "Skipping 'Liu_Tsung-chou' as it has no content.\n",
      "Skipping 'Pseudo-Grosseteste' as it has no content.\n",
      "Skipping 'Joseph_Kaspi' as it has no content.\n",
      "Skipping 'Márcio_Bilharinho_Naves' as it has no content.\n",
      "Skipping 'Minagawa_Kien' as it has no content.\n",
      "Skipping 'James_Oswald_(philosopher)' as it has no content.\n",
      "Skipping 'William_Penbygull' as it has no content.\n",
      "Skipping 'Arthur_Liebert' as it has no content.\n",
      "Skipping 'Boris_Petrovich_Vysheslavtsev' as it has no content.\n",
      "Skipping 'Yi_Kan' as it has no content.\n",
      "Skipping 'Frantisek_Weyr' as it has no content.\n",
      "Skipping 'George_Turnball' as it has no content.\n",
      "Skipping 'Johannes_Sharpe' as it has no content.\n",
      "Skipping 'Gabriel_Vazquez' as it has no content.\n"
     ]
    }
   ],
   "source": [
    "# Delete and recreate the downloads directory\n",
    "if os.path.exists(DOWNLOADS_DIR):\n",
    "    shutil.rmtree(DOWNLOADS_DIR)  # Delete the directory and all its contents\n",
    "os.makedirs(DOWNLOADS_DIR, exist_ok=True)  # Recreate the directory\n",
    "\n",
    "wiki_links = [\"List of philosophers (A–C)\", \"List of philosophers (D–H)\", \"List of philosophers (I–Q)\", \"List of philosophers (R–Z)\"]\n",
    "title_links = []\n",
    "for wiki_link in wiki_links:\n",
    "  wiki_markup = getJsonResponse(wiki_link)\n",
    "  title_links.extend(findLinks(wiki_markup))\n",
    "#wiki_markup_a_c = getJsonResponse(\"List of philosophers (A–C)\")\n",
    "#wiki_markup_d_h = getJsonResponse(\"List of philosophers (D–H)\")\n",
    "#wiki_markup_i_q = getJsonResponse(\"List of philosophers (I–Q)\")\n",
    "#wiki_markup_r_z = getJsonResponse(\"List of philosophers (R–Z)\")\n",
    "\n",
    "#title_links = findLinks(wiki_markup_a_c)\n",
    "#title_links.extend(findLinks(wiki_markup_d_h))\n",
    "#title_links.extend(findLinks(wiki_markup_i_q))\n",
    "#title_links.extend(findLinks(wiki_markup_r_z))\n",
    "\n",
    "# Remove irrelevant links if they exist\n",
    "for unwanted in [\"List_of_philosophers\", \"Philosopher\", \"Stanford_Encyclopedia_of_Philosophy\", \"Encyclopedia_of_Philosophy\", \"Routledge_Encyclopedia_of_Philosophy\", \"The_Cambridge_Dictionary_of_Philosophy\", \"The_Oxford_Companion_to_Philosophy\"]:\n",
    "    if unwanted in title_links:\n",
    "        title_links.remove(unwanted)\n",
    "\n",
    "# Writing to files (warning this takes a while)\n",
    "invalid_links = []  # Track titles that could not be saved\n",
    "for title_link in title_links:\n",
    "  all_wikitext = getJsonResponse(title_link)\n",
    "  if not all_wikitext:\n",
    "    print(f\"Skipping '{title_link}' as it has no content.\")\n",
    "    invalid_links.append(title_link)  # Track invalid pages without modifying the list directly\n",
    "    continue\n",
    "  filename = os.path.join(DOWNLOADS_DIR, f\"{title_link}.txt\")\n",
    "  with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(all_wikitext) # save all the wikitext into one file\n",
    "\n",
    "# Remove invalid links from title_links after iteration\n",
    "title_links = [link for link in title_links if link not in invalid_links]\n",
    "# Saves the variable locally so you dont need to run the above code again for graph re-creation if files are already downloaded\n",
    "with open(TITLE_LINKS_FILE, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(title_links, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating the network \n",
    "Creates three graphs:\n",
    "1. `G`: The entire network\n",
    "2. `S`: The largest connected component in the network\n",
    "3. `S_undirected`: Undirected version of `S`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowing to create the graph without redownloading the entire dataset\n",
    "if os.path.exists(TITLE_LINKS_FILE):\n",
    "    # Load the file as the title_links variable\n",
    "    with open(TITLE_LINKS_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "        title_links = json.load(file)\n",
    "else:\n",
    "    print(\"title_links file not found. You need to run step 2 again\")\n",
    "\n",
    "files = [f for f in os.listdir(DOWNLOADS_DIR) if f.endswith(\".txt\")]\n",
    "\n",
    "outgoing_links = {}\n",
    "for file in files:\n",
    "  if not file.endswith(\".txt\"): # makes sure only the txt files are opened\n",
    "    continue\n",
    "  file_path = os.path.join(DOWNLOADS_DIR, file)\n",
    "  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    wikipage = f.read()\n",
    "    wikipage_links = findLinks(wikipage)\n",
    "    withoutExtension = os.path.splitext(file)[0]\n",
    "\n",
    "    for link in wikipage_links:\n",
    "      if link in title_links:\n",
    "        outgoing_links.setdefault(withoutExtension, []).append(link)\n",
    "\n",
    "G = nx.DiGraph()\n",
    "for page in title_links:\n",
    "  file_path = os.path.join(DOWNLOADS_DIR, f\"{page}.txt\")\n",
    "  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "  word_count = len(content.split()) # splits the content into words by whitespace.\n",
    "  G.add_node(page, contentlength=word_count)\n",
    "  for link in outgoing_links.get(page) or []:\n",
    "    G.add_edge(page, link)\n",
    "\n",
    "## Removing isolated nodes\n",
    "isolated_nodes = [node for node, degree in dict(G.degree()).items() if degree == 0]\n",
    "if (isolated_nodes): G.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "## Getting the largest connected component\n",
    "largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "S = G.subgraph(largest_cc).copy() # copying it to another variable\n",
    "S_undirected = S.to_undirected() # making it undirected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjvG2Qzn4JmN"
   },
   "source": [
    "### Prelimenary data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BGM681a4JXQ",
    "outputId": "afa6dd04-3905-4be8-c093-27f6fa610064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 1470 nodes and 11203 edges\n",
      "DiGraph with 1464 nodes and 11200 edges\n",
      "Total Size of Data: 48.54 MB\n",
      "Number of Rows (Philosophers): 1734\n",
      "Number of Nodes: 1464\n",
      "Number of Links: 11200\n"
     ]
    }
   ],
   "source": [
    "# General graph analysis\n",
    "print(G) # whole graph\n",
    "print(S) # largest connected component\n",
    "\n",
    "# Calculating total data size\n",
    "total_size = sum(\n",
    "    os.path.getsize(os.path.join(DOWNLOADS_DIR, f)) for f in os.listdir(DOWNLOADS_DIR) if f.endswith(\".txt\"))\n",
    "total_size_mb = total_size / (1024 * 1024)  # Convert bytes to MB\n",
    "print(f\"Total Size of Data: {total_size_mb:.2f} MB\")\n",
    "\n",
    "# Number of rows\n",
    "num_rows = len(title_links)\n",
    "print(f\"Number of Rows (Philosophers): {num_rows}\")\n",
    "\n",
    "# Number of nodes and links\n",
    "num_nodes = S.number_of_nodes()\n",
    "num_links = S.number_of_edges()\n",
    "print(f\"Number of Nodes: {num_nodes}\")\n",
    "print(f\"Number of Links: {num_links}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HcyLopX8zh4T"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
