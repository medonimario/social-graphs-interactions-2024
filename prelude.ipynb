{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Osh4WeXAlsqn"
   },
   "source": [
    "# `Prelude`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse  # to handle special characters in the title\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Set the directory to downloads\n",
    "DOWNLOADS_DIR = \"downloads\"\n",
    "TITLE_LINKS_FILE = \"title_links.json\"\n",
    "\n",
    "def getJsonResponse(title):\n",
    "  # Define the components of the query\n",
    "  baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "  action = \"action=query\"\n",
    "  title = f\"titles={urllib.parse.quote(title)}\"\n",
    "  content = \"prop=revisions&rvprop=content\"\n",
    "  dataformat = \"format=json\"\n",
    "  rvslots = \"rvslots=main\"\n",
    "\n",
    "  # Construct the query URL\n",
    "  query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat, rvslots)\n",
    "\n",
    "  try:\n",
    "    # Make the request to Wikipedia API\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "\n",
    "    # Check if the HTTP status is OK (200)\n",
    "    if wikiresponse.getcode() != 200:\n",
    "      print(f\"Error: Received non-200 HTTP status code {wikiresponse.getcode()}\")\n",
    "      return None\n",
    "\n",
    "    wikidata = wikiresponse.read()\n",
    "\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "      wikiJson = json.loads(wikidata)\n",
    "    except json.JSONDecodeError:\n",
    "      print(\"Error: Failed to decode JSON response\")\n",
    "      return None\n",
    "\n",
    "    # Get the page from the JSON response\n",
    "    page = next(iter(wikiJson['query']['pages'].values()))  # extract the single page\n",
    "\n",
    "    # Check if the page has revisions and extract the latest wikitext content\n",
    "    if 'revisions' in page and len(page['revisions']) > 0:\n",
    "      wikitext = page['revisions'][0]['slots']['main']['*']  # extract wikitext from \"main\" slot\n",
    "      return wikitext\n",
    "    else:\n",
    "      #print(f\"Error: Page '{title}' does not contain revisions.\")\n",
    "      return None\n",
    "\n",
    "  except urllib.error.URLError as e:\n",
    "    print(f\"Network error: {e.reason}\")\n",
    "    return None\n",
    "  except Exception as e:\n",
    "    print(f\"Unexpected error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "## Convert the list to link titles e.g. John McCain (fictional) => John_McCain_(fictional)\n",
    "def extract_title_link(match):\n",
    "  # Regular expression to match the content between [[ and | (the first part of the link)\n",
    "  title = re.search(r'\\[\\[([^\\|\\]]+)', match)\n",
    "  if title:\n",
    "    # Replace all whitespaces in the title with underscores\n",
    "    return title.group(1).replace(\" \", \"_\")\n",
    "  else:\n",
    "    print(\"ERROR FINDING \", match)\n",
    "    return None\n",
    "\n",
    "def findLinks(wikipage):\n",
    "  pattern = r'\\[{2}[\\w\\-\\s\\(\\)]*\\|?[\\w\\s\\-\\(\\)]*\\]{2}' ## regex for finding links e.g.: [[John McCain (fictional)|John McCain]]\n",
    "  matches = re.findall(pattern, wikipage)\n",
    "  # Convert the list to a set to keep only unique matches\n",
    "  unique_matches = set(matches)\n",
    "\n",
    "  links = [extract_title_link(unique_match) for unique_match in unique_matches]\n",
    "  return links\n",
    "\n",
    "def build_graph_from_files(path):\n",
    "    files = os.listdir(path)\n",
    "    outgoing_links = {}\n",
    "    pages = set()\n",
    "    \n",
    "    # Process each file in the directory to collect outgoing links and all pages\n",
    "    for file in files:\n",
    "        if not file.endswith(\".txt\"): \n",
    "            continue\n",
    "        \n",
    "        filepath = os.path.join(path, file)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            wikipage = f.read()\n",
    "            wikipage_links = findLinks(wikipage)\n",
    "            withoutExtension = os.path.splitext(file)[0]\n",
    "            pages.add(withoutExtension)  # Add the page to the set of all pages\n",
    "            \n",
    "            for link in wikipage_links:\n",
    "                if link + \".txt\" in files:  # Only consider links that exist as files\n",
    "                    outgoing_links.setdefault(withoutExtension, []).append(link)\n",
    "                    pages.add(link)  # Add the linked page to the set of all pages\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add all pages to the graph with the 'contentlength' attribute\n",
    "    for page in pages:\n",
    "        filename = os.path.join(path, f\"{page}.txt\")\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        word_count = len(content.split())\n",
    "        G.add_node(page, contentlength=word_count)\n",
    "    \n",
    "    # Add edges based on outgoing links\n",
    "    for page, links in outgoing_links.items():\n",
    "        for link in links:\n",
    "            G.add_edge(page, link)\n",
    "\n",
    "    # Remove isolated nodes\n",
    "    isolated_nodes = list(nx.isolates(G))\n",
    "    if isolated_nodes:\n",
    "        G.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "    # Get the largest connected component\n",
    "    if nx.is_weakly_connected(G):\n",
    "        S = G.copy()\n",
    "    else:\n",
    "        largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "        S = G.subgraph(largest_cc).copy()\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetching\n",
    "Purpose: Fetches philosopher data from their wikipedia pages and downloads the wikipedia pages as `{philosopher_name}.txt` in a `downloads/` directory.\n",
    "\n",
    "**NOTE: This takes a while to run! Also deletes all previous content in `downloads`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyQyH8oTlrun",
    "outputId": "37757d18-7b4f-4765-e628-7552f76d05fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1485 pages.\n",
      "Skipped 32 pages with no content.\n",
      "Skipped 249 redirect pages.\n"
     ]
    }
   ],
   "source": [
    "wiki_links = [\"List of philosophers (A–C)\", \"List of philosophers (D–H)\", \"List of philosophers (I–Q)\", \"List of philosophers (R–Z)\"]\n",
    "title_links = []\n",
    "\n",
    "verbose = False # Debug output during loops\n",
    "invalid_links = []  # Track titles that could not be saved\n",
    "redirect_links = []  # Track titles that are redirects\n",
    "\n",
    "# Delete and recreate the downloads directory\n",
    "if os.path.exists(DOWNLOADS_DIR):\n",
    "    shutil.rmtree(DOWNLOADS_DIR)  # Delete the directory and all its contents\n",
    "os.makedirs(DOWNLOADS_DIR, exist_ok=True)  # Recreate the directory\n",
    "\n",
    "\n",
    "for wiki_link in wiki_links:\n",
    "  wiki_markup = getJsonResponse(wiki_link)\n",
    "  title_links.extend(findLinks(wiki_markup))\n",
    "\n",
    "# Remove irrelevant links if they exist\n",
    "for unwanted in [\"List_of_philosophers\", \"Philosopher\", \"Stanford_Encyclopedia_of_Philosophy\", \"Encyclopedia_of_Philosophy\", \"Routledge_Encyclopedia_of_Philosophy\", \"The_Cambridge_Dictionary_of_Philosophy\", \"The_Oxford_Companion_to_Philosophy\"]:\n",
    "    if unwanted in title_links:\n",
    "        title_links.remove(unwanted)\n",
    "\n",
    "# Writing to files (warning this takes a while)\n",
    "for title_link in title_links:\n",
    "  all_wikitext = getJsonResponse(title_link)\n",
    "  if not all_wikitext:\n",
    "    if verbose: print(f\"Skipping '{title_link}' as it has no content.\")\n",
    "    invalid_links.append(title_link)  # Track invalid pages without modifying the list directly\n",
    "    continue\n",
    "  \n",
    "  # Skip if the content starts with #REDIRECT\n",
    "  if all_wikitext.strip().startswith(\"#REDIRECT\"):\n",
    "      if verbose: print(f\"Skipping '{title_link}' as it is a redirect.\")\n",
    "      redirect_links.append(title_link)  # Track redirect pages\n",
    "      continue\n",
    "  \n",
    "  filename = os.path.join(DOWNLOADS_DIR, f\"{title_link}.txt\")\n",
    "  with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(all_wikitext) # save all the wikitext into one file\n",
    "\n",
    "print(f\"Downloaded {len(title_links)} pages.\")\n",
    "print(f\"Skipped {len(invalid_links)} pages with no content.\")\n",
    "print(f\"Skipped {len(redirect_links)} redirect pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network \n",
    "Creates three graphs:\n",
    "1. `G`: The entire network\n",
    "2. `S`: The largest connected component in the network\n",
    "3. `S_undirected`: Undirected version of `S`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowing to create the graph without redownloading the entire dataset\n",
    "if os.path.exists(TITLE_LINKS_FILE):\n",
    "    # Load the file as the title_links variable\n",
    "    with open(TITLE_LINKS_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "        title_links = json.load(file)\n",
    "else:\n",
    "    print(\"title_links file not found. You need to run data fetching again\")\n",
    "\n",
    "files = [f for f in os.listdir(DOWNLOADS_DIR) if f.endswith(\".txt\")]\n",
    "\n",
    "outgoing_links = {}\n",
    "for file in files:\n",
    "  if not file.endswith(\".txt\"): # makes sure only the txt files are opened\n",
    "    continue\n",
    "  file_path = os.path.join(DOWNLOADS_DIR, file)\n",
    "  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    wikipage = f.read()\n",
    "    wikipage_links = findLinks(wikipage)\n",
    "    withoutExtension = os.path.splitext(file)[0]\n",
    "\n",
    "    for link in wikipage_links:\n",
    "      if link in title_links:\n",
    "        outgoing_links.setdefault(withoutExtension, []).append(link)\n",
    "\n",
    "G = nx.DiGraph()\n",
    "for page in title_links:\n",
    "  file_path = os.path.join(DOWNLOADS_DIR, f\"{page}.txt\")\n",
    "  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "  word_count = len(content.split()) # splits the content into words by whitespace.\n",
    "  G.add_node(page, contentlength=word_count)\n",
    "  for link in outgoing_links.get(page) or []:\n",
    "    G.add_edge(page, link)\n",
    "\n",
    "## Removing isolated nodes\n",
    "isolated_nodes = [node for node, degree in dict(G.degree()).items() if degree == 0]\n",
    "if (isolated_nodes): G.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "## Getting the largest connected component\n",
    "largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "S = G.subgraph(largest_cc).copy() # copying it to another variable\n",
    "S_undirected = S.to_undirected() # making it undirected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjvG2Qzn4JmN"
   },
   "source": [
    "## Prelimenary data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BGM681a4JXQ",
    "outputId": "afa6dd04-3905-4be8-c093-27f6fa610064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 1376 nodes and 10856 edges\n",
      "DiGraph with 1366 nodes and 10850 edges\n",
      "Total Size of Data: 48.53 MB\n",
      "Number of Rows (Philosophers): 1485\n",
      "Number of Nodes: 1366\n",
      "Number of Links: 10850\n"
     ]
    }
   ],
   "source": [
    "# General graph analysis\n",
    "print(G) # whole graph\n",
    "print(S) # largest connected component\n",
    "\n",
    "# Calculating total data size\n",
    "total_size = sum(\n",
    "    os.path.getsize(os.path.join(DOWNLOADS_DIR, f)) for f in os.listdir(DOWNLOADS_DIR) if f.endswith(\".txt\"))\n",
    "total_size_mb = total_size / (1024 * 1024)  # Convert bytes to MB\n",
    "print(f\"Total Size of Data: {total_size_mb:.2f} MB\")\n",
    "\n",
    "# Number of rows\n",
    "num_rows = len(title_links)\n",
    "print(f\"Number of Rows (Philosophers): {num_rows}\")\n",
    "\n",
    "# Number of nodes and links\n",
    "num_nodes = S.number_of_nodes()\n",
    "num_links = S.number_of_edges()\n",
    "print(f\"Number of Nodes: {num_nodes}\")\n",
    "print(f\"Number of Links: {num_links}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HcyLopX8zh4T"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
