{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Osh4WeXAlsqn"
   },
   "source": [
    "# Prelude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains the following:\n",
    "1. Imports and defining helper functions\n",
    "2. Scraping data from wikipedia and downloads it\n",
    "3. Building the network either from\n",
    "    - (A) Downloaded files\n",
    "    - (B) Local pickle file (created from last time A was run)\n",
    "4. Simple prelimenary data analysis of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse  # to handle special characters in the title\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# Set the directory to downloads\n",
    "DOWNLOADS_DIR = \"downloads\"\n",
    "TITLE_LINKS_FILE = \"title_links.json\"\n",
    "\n",
    "def getJsonResponse(title):\n",
    "  # Define the components of the query\n",
    "  baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "  action = \"action=query\"\n",
    "  title = f\"titles={urllib.parse.quote(title)}\"\n",
    "  content = \"prop=revisions&rvprop=content\"\n",
    "  dataformat = \"format=json\"\n",
    "  rvslots = \"rvslots=main\"\n",
    "\n",
    "  # Construct the query URL\n",
    "  query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat, rvslots)\n",
    "\n",
    "  try:\n",
    "    # Make the request to Wikipedia API\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "\n",
    "    # Check if the HTTP status is OK (200)\n",
    "    if wikiresponse.getcode() != 200:\n",
    "      print(f\"Error: Received non-200 HTTP status code {wikiresponse.getcode()}\")\n",
    "      return None\n",
    "\n",
    "    wikidata = wikiresponse.read()\n",
    "\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "      wikiJson = json.loads(wikidata)\n",
    "    except json.JSONDecodeError:\n",
    "      print(\"Error: Failed to decode JSON response\")\n",
    "      return None\n",
    "\n",
    "    # Get the page from the JSON response\n",
    "    page = next(iter(wikiJson['query']['pages'].values()))  # extract the single page\n",
    "\n",
    "    # Check if the page has revisions and extract the latest wikitext content\n",
    "    if 'revisions' in page and len(page['revisions']) > 0:\n",
    "      wikitext = page['revisions'][0]['slots']['main']['*']  # extract wikitext from \"main\" slot\n",
    "      return wikitext\n",
    "    else:\n",
    "      #print(f\"Error: Page '{title}' does not contain revisions.\")\n",
    "      return None\n",
    "\n",
    "  except urllib.error.URLError as e:\n",
    "    print(f\"Network error: {e.reason}\")\n",
    "    return None\n",
    "  except Exception as e:\n",
    "    print(f\"Unexpected error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "## Convert the list to link titles e.g. John McCain (fictional) => John_McCain_(fictional)\n",
    "def extract_title_link(match):\n",
    "  # Regular expression to match the content between [[ and | (the first part of the link)\n",
    "  title = re.search(r'\\[\\[([^\\|\\]]+)', match)\n",
    "  if title:\n",
    "    # Replace all whitespaces in the title with underscores\n",
    "    return title.group(1).replace(\" \", \"_\")\n",
    "  else:\n",
    "    print(\"ERROR FINDING \", match)\n",
    "    return None\n",
    "\n",
    "def findLinks(wikipage):\n",
    "  pattern = r'\\[{2}[\\w\\-\\s\\(\\)]*\\|?[\\w\\s\\-\\(\\)]*\\]{2}' ## regex for finding links e.g.: [[John McCain (fictional)|John McCain]]\n",
    "  matches = re.findall(pattern, wikipage)\n",
    "  # Convert the list to a set to keep only unique matches\n",
    "  unique_matches = set(matches)\n",
    "\n",
    "  links = [extract_title_link(unique_match) for unique_match in unique_matches]\n",
    "  return links\n",
    "\n",
    "def build_graph_from_files(path):\n",
    "    files = os.listdir(path)\n",
    "    outgoing_links = {}\n",
    "    pages = set()\n",
    "    \n",
    "    # Process each file in the directory to collect outgoing links and all pages\n",
    "    for file in files:\n",
    "        if not file.endswith(\".txt\"): \n",
    "            continue\n",
    "        \n",
    "        filepath = os.path.join(path, file)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            wikipage = f.read()\n",
    "            wikipage_links = findLinks(wikipage)\n",
    "            withoutExtension = os.path.splitext(file)[0]\n",
    "            pages.add(withoutExtension)  # Add the page to the set of all pages\n",
    "            \n",
    "            for link in wikipage_links:\n",
    "                if link + \".txt\" in files:  # Only consider links that exist as files\n",
    "                    outgoing_links.setdefault(withoutExtension, []).append(link)\n",
    "                    pages.add(link)  # Add the linked page to the set of all pages\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add all pages to the graph with the 'contentlength' attribute\n",
    "    for page in pages:\n",
    "        filename = os.path.join(path, f\"{page}.txt\")\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        word_count = len(content.split())\n",
    "        G.add_node(page, contentlength=word_count)\n",
    "    \n",
    "    # Add edges based on outgoing links\n",
    "    for page, links in outgoing_links.items():\n",
    "        for link in links:\n",
    "            G.add_edge(page, link)\n",
    "\n",
    "    # Remove isolated nodes\n",
    "    isolated_nodes = list(nx.isolates(G))\n",
    "    if isolated_nodes:\n",
    "        G.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "    # Get the largest connected component\n",
    "    if nx.is_weakly_connected(G):\n",
    "        S = G.copy()\n",
    "    else:\n",
    "        largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "        S = G.subgraph(largest_cc).copy()\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping data\n",
    "Fetches philosopher data from their wikipedia pages and downloads the wikipedia pages as `{philosopher_name}.txt` in a `downloads/` directory.\n",
    "\n",
    ">**NOTES**\n",
    "> 1. This takes a while to run\n",
    "> 2. It deletes all previous content in `downloads`\n",
    "> 3. Downloads all pages but skips pages with *no content* or *redirects*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyQyH8oTlrun",
    "outputId": "37757d18-7b4f-4765-e628-7552f76d05fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1485 pages.\n",
      "Skipped 32 pages with no content.\n",
      "Skipped 249 redirect pages.\n"
     ]
    }
   ],
   "source": [
    "wiki_links = [\"List of philosophers (A–C)\", \"List of philosophers (D–H)\", \"List of philosophers (I–Q)\", \"List of philosophers (R–Z)\"]\n",
    "title_links = []\n",
    "\n",
    "verbose = False # Debug output during loops\n",
    "invalid_links = []  # Track titles that could not be saved\n",
    "redirect_links = []  # Track titles that are redirects\n",
    "\n",
    "# Delete and recreate the downloads directory\n",
    "if os.path.exists(DOWNLOADS_DIR):\n",
    "    shutil.rmtree(DOWNLOADS_DIR)  # Delete the directory and all its contents\n",
    "os.makedirs(DOWNLOADS_DIR, exist_ok=True)  # Recreate the directory\n",
    "\n",
    "\n",
    "for wiki_link in wiki_links:\n",
    "  wiki_markup = getJsonResponse(wiki_link)\n",
    "  title_links.extend(findLinks(wiki_markup))\n",
    "\n",
    "# Remove irrelevant links if they exist\n",
    "for unwanted in [\"List_of_philosophers\", \"Philosopher\", \"Stanford_Encyclopedia_of_Philosophy\", \"Encyclopedia_of_Philosophy\", \"Routledge_Encyclopedia_of_Philosophy\", \"The_Cambridge_Dictionary_of_Philosophy\", \"The_Oxford_Companion_to_Philosophy\"]:\n",
    "    if unwanted in title_links:\n",
    "        title_links.remove(unwanted)\n",
    "\n",
    "# Writing to files (warning this takes a while)\n",
    "for title_link in title_links:\n",
    "  all_wikitext = getJsonResponse(title_link)\n",
    "  if not all_wikitext:\n",
    "    if verbose: print(f\"Skipping '{title_link}' as it has no content.\")\n",
    "    invalid_links.append(title_link)  # Track invalid pages without modifying the list directly\n",
    "    continue\n",
    "  \n",
    "  # Skip if the content starts with #REDIRECT\n",
    "  if all_wikitext.strip().startswith(\"#REDIRECT\"):\n",
    "      if verbose: print(f\"Skipping '{title_link}' as it is a redirect.\")\n",
    "      redirect_links.append(title_link)  # Track redirect pages\n",
    "      continue\n",
    "  \n",
    "  filename = os.path.join(DOWNLOADS_DIR, f\"{title_link}.txt\")\n",
    "  with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(all_wikitext) # save all the wikitext into one file\n",
    "\n",
    "title_links = [link for link in title_links if link not in invalid_links + redirect_links]\n",
    "print(f\"Downloaded {len(title_links)} pages.\")\n",
    "print(f\"Skipped {len(invalid_links)} pages with no content.\")\n",
    "print(f\"Skipped {len(redirect_links)} redirect pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A) Create from scratch \n",
    "From `downloads/`directory (saves local pickle file for later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = build_graph_from_files(DOWNLOADS_DIR)\n",
    "pickle.dump(S, open(\"graph.pkl\", \"wb\")) # Saved as local version for later use for (B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) OR use local version \n",
    "From `pickle` file created last time you ran (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph:\n",
    "S = pickle.load(open(\"graph.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjvG2Qzn4JmN"
   },
   "source": [
    "## 4. Prelimenary data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1366\n",
      "Number of edges: 10850\n",
      "Size of downloaded data: 48.80 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of nodes: {S.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {S.number_of_edges()}\")\n",
    "\n",
    "# Calculating total data size\n",
    "download_size = sum(os.path.getsize(os.path.join(DOWNLOADS_DIR, f)) for f in os.listdir(DOWNLOADS_DIR) if f.endswith(\".txt\"))\n",
    "download_size_mb = download_size / (1024 * 1024)  # Convert bytes to MB\n",
    "print(f\"Size of downloaded data: {download_size_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HcyLopX8zh4T"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "course02502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
